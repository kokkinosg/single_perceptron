{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80863b31",
   "metadata": {},
   "source": [
    "# [INSERT PROJECT TITLE]\n",
    "\n",
    "[INSERT ROJECT DESCRIPTION]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7aca32",
   "metadata": {},
   "source": [
    "## 1.0: Data ingestion\n",
    "\n",
    "Load the MNIST handwritten digits data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab80d5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "mnist = keras.datasets.mnist\n",
    "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c4d9b4",
   "metadata": {},
   "source": [
    "### 1.1: Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9384799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. training data = 60000\n",
      "No. test data = 10000\n",
      "Each training/test data is an image of 28 x 28 bits.\n",
      "Y data correspond to a digit which is depicted in a corresponding 28 x 28 image. For example, for 2nd image, y = 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"No. training data = {x_train.shape[0]}\")\n",
    "print(f\"No. test data = {x_test.shape[0]}\")\n",
    "print(f\"Each training/test data is an image of {x_train.shape[1]} x {x_train.shape[2]} bits.\")\n",
    "print(f\"Y data correspond to a digit which is depicted in a corresponding 28 x 28 image. For example, for 2nd image, y = {y_train[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d43926",
   "metadata": {},
   "source": [
    "## 2.0: Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd6d6dd",
   "metadata": {},
   "source": [
    "### 2.1: Normalise pixel values\n",
    "\n",
    "Each pixel is [0,255]. We need to make it symmetric around 0 and make the max and min smaller so our MLP trains more effectively. \n",
    "\n",
    "This means we have to convert them from [0,255] to [-1.1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d887f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 127.5 - 1, x_test / 127.5 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c83e2",
   "metadata": {},
   "source": [
    "### 2.2: Flatten the input data\n",
    "\n",
    "A Multi-Layer Perceptron (MLP) does not explicitly model spatial relationships in images. Instead, it expects the input to be a one-dimensional feature vector. Therefore, each image must be flattened before being passed to the network. This means that instead of x_train.shape = (60000,28,28), it should be (60000,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f46f4c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After flattening, the x_train shape = (60000, 784)\n",
      "After flattening, the x_test shape = (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Find the total number of pixels \n",
    "nb_features = np.prod(x_train.shape[1:])\n",
    "\n",
    "# Change the shape from (n_train,28,28) to (n_train,784)\n",
    "n_train = x_train.shape[0]\n",
    "x_train.resize((n_train, nb_features))\n",
    "print(f\"After flattening, the x_train shape = {x_train.shape}\")\n",
    "\n",
    "# Change shape of test data \n",
    "n_test = x_test.shape[0]\n",
    "x_test.resize((n_test, nb_features))\n",
    "print(f\"After flattening, the x_test shape = {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf84014c",
   "metadata": {},
   "source": [
    "## 3.0 PCA - NOT YET IMPLEMENTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff7163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3ea5d04",
   "metadata": {},
   "source": [
    "## 4.0 Perceptrons\n",
    "\n",
    "We will implement an iterative algorithm for training the single layer perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3440c067",
   "metadata": {},
   "source": [
    "### 4.1 Filter data for binary classifier\n",
    "\n",
    "As we are dealing with a binary classification problem, we will pick data points corresponding to classes 0 and 1 (handwritten digits). In  addition, we choose our binary labels to be -1 and 1, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c9b2331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Create a condition mask. If the label is 1 or 0, it outputs True, otherwise False. \n",
    "cond = (y_train == 0) | (y_train == 1) \n",
    "\n",
    "# Extract all elements from x_train and y_train where the corresponding element from y_train satsifies the condition.\n",
    "binary_x_train = x_train[cond,:]\n",
    "binary_y_train = y_train[cond] # y_train is 1D\n",
    "\n",
    "# COnvert the labels to float. By defualt its is uint8 which doesn't accept negative numbers.\n",
    "binary_y_train = binary_y_train.astype(float)\n",
    "\n",
    "# Convert all labels of 0 to -1 \n",
    "binary_y_train[binary_y_train == 0] = -1\n",
    "\n",
    "print(type(binary_x_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8815e92",
   "metadata": {},
   "source": [
    "### 4.2 Create a prediction function \n",
    "\n",
    "The predict function multiplies each input value by a weight and adds them together, along with a bias term. It passes the result through an activation function. The output is either -1 or 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a55c1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x: np.ndarray, w: np.ndarray, b: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        x (np.ndarray): Input matrix which is of size n x m. Where n is the number of data points and m is the feature dimensionality (i.e. 784)\n",
    "        w (np.ndarray): The vector of weights of size m (i,.e. 784). This is what we need to learn. Here it is 784 elements long. \n",
    "        b (float): The bias term. \n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A vector of size m, containing prediction values for x\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the weighted sum\n",
    "    weighted_sum = x @ w\n",
    "\n",
    "    # Add the bias term\n",
    "    weighted_sum_bias = weighted_sum + b\n",
    "\n",
    "    # Pass it through a sign fucntion to get the prediction\n",
    "    prediction = np.sign(weighted_sum_bias)\n",
    "\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c67ba1",
   "metadata": {},
   "source": [
    "### 4.3 Single layer perceptron \n",
    "\n",
    "We will use function \"predict\" to implement a single layer perceptron algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9078af11",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m (x: \u001b[43mnp\u001b[49m.ndarray, y: np.ndarray) -> \u001b[38;5;28mtuple\u001b[39m[np.ndarray, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    Function which based on inputs and corresponding labels, optimises the weights and the bias. \u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[33;03m        list[float]: Error list for plotting. NOTE that it skips the inifnite error for iter 0. \u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Define local constants \u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def optimize (x: np.ndarray, y: np.ndarray) -> tuple[np.ndarray, float, float, list[float]]:\n",
    "    \"\"\"\n",
    "    Function which based on inputs and corresponding labels, optimises the weights and the bias. \n",
    "\n",
    "    Parameters:\n",
    "        x (np.ndarray): Input matrix which is of size n x m. Where n is the number of data points and m is the feature dimensionality (i.e. 784)\n",
    "        y (np.ndarray): A vector of size m, containing prediction values for x\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Optimised parameter vector\n",
    "        float: Corresponding learned bias\n",
    "        float: Classification error\n",
    "        list[float]: Error list for plotting. NOTE that it skips the inifnite error for iter 0. \n",
    "    \"\"\"\n",
    "    # Define local constants \n",
    "    LEARNING_RATE = 0.1 \n",
    "    n, m = x.shape # Extract the number of datapoints (vectors) and their size (i.e. 784)\n",
    "\n",
    "    # Initialise iterator and error and error list\n",
    "    iter = 0\n",
    "    error = np.inf # Error initially is infinity\n",
    "    error_list = [] # This is a list of errors to allow plotting. \n",
    "\n",
    "    # Initialise weights and initial bias\n",
    "    w = np.random.rand(m) # Create an array with m elements and populate it with random samples from a uniform distribution over 0,1\n",
    "    b = np.random.rand() # Single random float for initial bias\n",
    "\n",
    "    # Run the optimising loop until 1000 iteration OR error falls below 0.001\n",
    "    while (iter <= 1000) & (error > 1e-3):\n",
    "        \n",
    "        # Reset number of wrong predictions for each iteration \n",
    "        num_wrong_predictions = 0\n",
    "\n",
    "        # Get the prediction matrix \n",
    "        prediction = predict(x,w,b) \n",
    "\n",
    "        # Go over each datapoint \n",
    "        for i in range (n):\n",
    "\n",
    "            # If the prediction is wrong, update weights, bias and error \n",
    "            if prediction[i] != y[i]:\n",
    "                w = w + LEARNING_RATE * y[i] * x[i] # w = w + ηy[i]x[i]\n",
    "                b = b + LEARNING_RATE * y[i] # b = b + ηy[i]\n",
    "                num_wrong_predictions += 1\n",
    "        \n",
    "        # Calculate error for the iteration. Error = Num of Wrong Prediction / Total number of samples\n",
    "        error = num_wrong_predictions / n\n",
    "        \n",
    "        # Add the iteration error to the list. \n",
    "        error_list.append(error)\n",
    "\n",
    "        # Increase the iterator\n",
    "        iter += 1\n",
    "    \n",
    "    return w, b, error, error_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
